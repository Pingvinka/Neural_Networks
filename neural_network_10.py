# -*- coding: utf-8 -*-
"""Neural_Network_10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LPzkclTVYplr5Qh76fovRdll8as1mIGr

# Загрузка всего необходимого
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM, Dropout, Input, BatchNormalization, Attention
from tensorflow.keras import utils
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.saving import save_model, load_model
from sklearn.model_selection import train_test_split
import tensorflow_hub as hub

#Загрузка данных для обучения и тестирования
data = pd.read_csv('/content/Train.csv', on_bad_lines='skip', delimiter='\t')
X_need = pd.read_csv('/content/Test.csv', on_bad_lines='skip', delimiter='\t')
submission = pd.read_csv('/content/submission.csv', on_bad_lines='skip', delimiter='\t')

data

"""# Подготовка данных"""

def posneg(stroka):
  if stroka == 'Positive':
    return 1
  else:
    return 0

#Предобработка: удаление пропусков (их тут нет), преобразование категориальных признаков в вектор (для softmax. Можно было просто к значениям 0/1, тогда в инициализации модели в последнем слое используем сигмоидальную функцию)
data = data.drop('idx', axis=1)
data['Score'] = data['Score'].apply(posneg)
data

data.iloc[7,1]

X_need = X_need.drop('idx', axis=1)

Train, Test = train_test_split(data, test_size=0.2, random_state=42)

#Токенизация
texts = Train['Text']
tokenizer = Tokenizer(num_words=2000, filters='!–"—#$%&amp;()*+,-./:;<=>?@[\\]^_`{|}~\t\n\r«»', lower=True, split=' ', char_level=False) #Создаем токенизатор, не хочу слишком долго обучать модель, поэтому ставлю 2000
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts) #векторизация
X_train = pad_sequences(sequences, maxlen = 350) #устанавливаем определенную длину
y_train = Train['Score'].to_numpy()

#Повторяем всё то же самое с тестовым набором
texts = Test['Text']
sequences = tokenizer.texts_to_sequences(texts) #векторизация
X_test = pad_sequences(sequences, maxlen = 350)
y_test = Test['Score'].to_numpy()

X_train, y_train, X_test, y_test #Смотрю, всё ли хорошо

"""# Создание и обучение нейронной сети"""

#Инициализация
model = Sequential()

model.add(Input(shape=(350,)))
model.add(Embedding(2000, 128))
model.add(GRU(128, return_sequences=True))
model.add(GRU(128))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.summary()

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics = ['accuracy', 'mae'])

callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1),
             ModelCheckpoint(filepath='best_modelG.h5.keras', monitor='val_loss', save_best_only=True)]

#Обучение
history = model.fit(X_train,
                    y_train,
                    epochs=15,
                    batch_size=32,
                    validation_split=0.2,
                    callbacks=callbacks)

plt.plot(history.history['accuracy'], label = 'Верные ответы при обучении')
plt.plot(history.history['val_accuracy'], label = 'Верные ответы при проверке')
plt.xlabel('эпоха')
plt.ylabel('верные ответы')
plt.legend()
plt.show()

save_model(model, 'best_modelG.h5.keras')

"""# Тестирование нейронной сети

"""

modelG = load_model('best_modelG.h5.keras')

modelG.evaluate(X_test, y_test, verbose=1)

t = "Обслуживаюсь на Цветном Бульваре. Продукт - кредитная карта. Все ясно и четко. Грамотные работники колл-центра - так держать. Являюсь клиентом уже 3-й год. Очень доволен - молодцы!В прошлом году была просрочка по платежам месяца полтора, истратил весь лимит - была сложная фин. ситуация и смена работы. Никто не парил мозг звонками - позвонили один раз, в ОЧЕНЬ ТАКТИЧНОЙ И ВЕЖЛИВОЙ форме напомнили о просрочке, спросили, когда БУДЕТ ВОЗМОЖНОСТЬ погасить задолженность. Ответил - через 3 недели. Все ОК, ждем. Больше НИ ОДНОГО звонка.Была проблема с оплатой через интернет. Позвонил. Ответили, что данный ресурс заблокирован и операция отменена банком. Попросил разблокировать - все решили в течение 10 минут. Вежливые операционисты. Поймал себя на том, что когда звоню в колл-центр, то звоню к людям, которые помогут решить мою проблему, а не с негативом и желанием ругаться уже заранее, как в некоторые другие банковские заведения.В общем, спасибо за хороший сервис и грамотных специалистов!".lower() #Чисто для себя проверить
data = tokenizer.texts_to_sequences([t])
data_pad = pad_sequences(data, maxlen=300)

res = modelG.predict(data_pad)
print(res)

"""# Для олимпиады"""

#modelG =

#Повторяем всё то же самое с тестовым набором
texts = X_need['Text']
sequences = tokenizer.texts_to_sequences(texts) #векторизация
X_need = pad_sequences(sequences, maxlen = 350)

y_need = modelG.predict(X_need)

#Организаторы требуют файл csv
y_need[5998] #Работает отлично! Почти идеально

y_need

threshold = 0.5
y_need = np.where(y_need >= threshold, 'Positive', 'Negative')
print(y_need)

pd.set_option('display.max_colwidth', None)
X_need

idx = submission['idx']

otvet = pd.Series(y_need.reshape(6000, ))
otvet = otvet.rename('Score')

olimp = pd.concat([idx, otvet], axis=1)
olimp

olimp.to_csv('Olimp_G.csv', index=False, sep='\t')