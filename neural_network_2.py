# -*- coding: utf-8 -*-
"""Neural_Network_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ai3aVYhpWq1qDBoUnFY5A1tlpWWBIKRa

Задача бинарной классификации

База данных: банки, кредитование, Kaggle

#Загрузка данных
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from keras.activations import softmax
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Attention, Dropout, BatchNormalization
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

data = pd.read_csv('/content/bank-full.csv', delimiter = ';')
data

"""#Предобработка данных"""

data.isna().sum() #Пропуски не обнаружены
data['y'].value_counts(normalize=True) #Невероятно плохо масштабированы данные. 88 'no'/12 'yes'

kategorical_data = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']  #Создаём список из названий исключительно категориальных признаков
for kat_priznak in kategorical_data:
  lst = data[kat_priznak].unique()
  slovar_stolb = {}
  for i in range(0, len(lst)):
    slovar_stolb.update({lst[i]: i})
  data[kat_priznak] = data[kat_priznak].map(slovar_stolb)
data

X = data.drop('y', axis=1)
y = data['y']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""#Создание и обучение нейронной сети"""

model = Sequential()
model.add(Dense(256, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam',
              loss = 'binary_crossentropy',
              metrics = ['AUC'])

history = model.fit(X_train,
                    y_train,
                    epochs=10,
                    batch_size=32,
                    validation_split=0.2,
                    verbose=1)

plt.plot(history.history['loss'], label = 'Ошибка при обучении')
plt.plot(history.history['val_loss'], label = 'Ошибка при проверке')
plt.xlabel('эпоха')
plt.ylabel('верные ответы')
plt.legend()
plt.show()

plt.plot(history.history['auc'], label = 'Верные ответы при обучении')
plt.plot(history.history['val_auc'], label = 'Верные ответы при проверке')
plt.xlabel('эпоха')
plt.ylabel('верные ответы')
plt.legend()
plt.show()

"""#Тестирование нейронной сети"""

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}")