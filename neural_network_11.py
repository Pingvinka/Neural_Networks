# -*- coding: utf-8 -*-
"""Neural_Network_11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bZWsXZZU8H7XWnIUTCIzHq9RqwnZ-ZBm

Глубокие рекуррентные нейронные сети

Задача: определение категории на картинке

База данных: олимпиада по ИИ

#Загрузка данных
"""

import matplotlib as plt
import numpy as np
import pandas as pd
import os
import re
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Embedding, GRU, LSTM, Input, Concatenate, Flatten
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.applications import VGG16, ResNet50
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import plot_model
from PIL import Image
from tensorflow.keras.preprocessing import image

! unzip Data.zip

Train = pd.read_csv('/content/Train.csv', on_bad_lines= 'skip', delimiter=';')
X_need = pd.read_csv('/content/Test.csv', on_bad_lines= 'skip', delimiter=';')
submission = pd.read_csv('/content/submission.csv', on_bad_lines= 'skip', delimiter=';')

Train

def file_to_jpg(path):
  for filename in os.listdir(path):
    new_filename = filename + '.jpg'
    os.rename(os.path.join(path, filename), os.path.join(path, new_filename))


path1 = '/content/Data/Test'
path2 = '/content/Data/Train'
file_to_jpg(path1)
file_to_jpg(path2)

print("Все файлы переименованы.")

'''
train_path = "/content/Data/Test"

for filename in os.listdir(train_path):
    if filename.endswith(".jpg"):
        new_filename = filename[:-4]
        os.rename(os.path.join(train_path, filename), os.path.join(train_path, new_filename))'''

"""#Предобработка данных"""

#Оставляем столбцы с нужными значениями
needed = ['Развлечения и юмор', "Кулинария", "Торговля и объявления", "СМИ", "Философия и религия", "Животные", "Творчество и дизайн", "Путешествия"]
Train = Train[Train['label'].isin(needed)]
Train

def to_jpg(id):
    return str(id) +".jpg"

Train['id'] = Train['id'].apply(to_jpg)
X_need['id'] = X_need['id'].apply(to_jpg)

description = Train['description']
Train.drop('description', axis=1)

ids = np.array(Train['id'])

train_dir = '/content/Data/Train'
test_dir = '/content/Data/Test'

img_width, img_height = 256, 256

# Функция для загрузки и предобработки изображения
def load_and_preprocess_image(img_path):
    img = Image.open(img_path)
    img = img.resize((img_width, img_height)) 
    img = image.img_to_array(img)
    img = img / 255.0
    return img

# Загрузка тренировочных данных
X_train = []
for filename in os.listdir(train_dir):
    if filename in ids:
      img_path = os.path.join(train_dir, filename)
      img = load_and_preprocess_image(img_path)
      X_train.append(img)

X_train_0 = X_train

images_1ch = []
images_hz_ch = []
images_3ch = []
images_4ch = []
for i in range(len(X_train_0)):
    img = X_train_0[i]
    if img.shape[1] == 1:
        images_1ch.append(i)
    elif img.shape[2] == 4:
        img = img[:, :, :3]
    elif img.shape[2] == 1:
        img = np.tile(img, (1, 1, 3))
    X_train_0[i] = img

for i in X_train_0:
  print(np.shape(i))

type(X_train_0)

X_train_0 = np.asarray(X_train_0)

unique_targets = Train['label'].unique()
label_encoder = LabelEncoder()
label_encoder.fit(unique_targets)
encoded_targets = label_encoder.transform(Train['label'])

labels = to_categorical(encoded_targets, 8)

"""Дальше предобработка текста. Нужен ли он вообще???"""

description =  description.astype(str)
description = description.fillna('') #Не будет ли это ошибкой????????????

#Токенизация
texts = description
tokenizer = Tokenizer(num_words=2500, filters='!–"—#$%&amp;()*+,-./:;<=>?@[\\]^_`{|}~\t\n\r«»', lower=True, split=' ', char_level=False) #Создаем токенизатор, не хочу слишком долго обучать модель, поэтому ставлю 2000
tokenizer.fit_on_texts(texts)
dist = list(tokenizer.word_counts.items()) #Для визуализации
print(dist[:10])

description = tokenizer.texts_to_sequences(texts) #векторизация
X_train_1 = pad_sequences(description, maxlen = 150) #устанавливаем определенную длину
X_train_1

texts = X_need['description']
sequences = tokenizer.texts_to_sequences(texts) #векторизация
X_test_1 = pad_sequences(sequences, maxlen = 150)
X_test_1

np.shape(X_train_1)

np.shape(X_train_0)

"""#Создание и обучение нейронной сети"""

input_1 = Input((256, 256, 3))
x_1 = Conv2D(16, (3,3))(input_1)
x_1 = MaxPooling2D()(x_1)
x_1 = Conv2D(32, (3,3))(x_1)
x_1 = MaxPooling2D()(x_1)
x_1 = Conv2D(64, (3,3))(x_1)
x_1 = MaxPooling2D()(x_1)
x_1 = Conv2D(128, (3,3))(x_1)
x_1 = MaxPooling2D()(x_1)
x_1 = Flatten()(x_1)
x_1 = Dense(128, activation = 'relu')(x_1)
x_1 = Dense(64, activation = 'relu')(x_1)

input_2 = Input((150,))
x_2 = Embedding(2500, 256)(input_2)
x_2 = GRU(128, return_sequences=True)(x_2)
x_2 = GRU(128)(x_2)
x_2 = Dense(64, activation = 'relu')(x_2)

x = Concatenate()([x_1, x_2])
x = Dense(64, activation = 'relu')(x)
output = Dense(8, activation = 'softmax')(x)

model = Model(inputs=[input_1, input_2], outputs=output)
model.summary()

plot_model(model, to_file='model.png', show_shapes=True)

model.compile(optimizer='adam',
              loss='categorical_crossentopy',
              metrics = ['Accuracy', 'AUC'])

callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1),
             ModelCheckpoint(filepath='best_modelO.h5.keras', monitor='val_loss', save_best_only=True)]

history = model.fit([X_train_0, X_train_1],
                    labels,
                    epochs=5,
                    batch_size=32,
                    validation_split=0.2,
                    callbacks=callbacks)

"""#Дообучение Resnet50/VGG16"""









"""#Тестирование нейронной сети"""



